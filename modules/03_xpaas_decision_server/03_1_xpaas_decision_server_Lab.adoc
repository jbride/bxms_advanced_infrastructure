:scrollbar:
:data-uri:
:toc2:
:numbered:
:ocdownload: link:https://access.redhat.com/downloads/content/290/ver=3.3/rhel---7/3.3.0.35/x86_64/product-software[OpenShift CLI client]

= xPaaS Decision Server Lab

*Goals*

In this module of the course, we will explore the Red Hat xPaas Decision Server. The Decision Server runtime is a BRMS KIE Server configured for deployment on OpenShift. The Decision Server supports source-to-image builds, as well as binary deployments starting from pre-built kjars.

In this lab, we explore the source-to-image (S2I) functionality of the OpenShift xPaas Decision Server.
Goals are as follows:

* Provisioning of the Red Hat BPM Decision Server in an OpenShift Container Platform (OCP) environment.
* Deployment of a rules based _kie-project_ to the Decision Server.
* Automated build and redeployment when code changes are made to our _kie-project_.
* Investigation of various OCP Deployment Strategies so as to reduce down-time.

*Prerequisites*

In order to do this lab you will need :

* Red Hat GPTE OPENTLC SSO Id
* The following tools on your local workstation:
** {ocdownload} (`oc`) v3.3 (or more recent)
** git
** maven (v3.3.0 or more recent)
** curl.
+
NOTE: The lab virtual machine used in this course has these tools installed.
* Access to Github to clone the projects used in this lab.
* Completion of Red Hat GPTE's _Application Development using OpenShift_ course.
* Experience with the Red Hat BRMS product; in particular, the _kie-server_ API.
* Experience using Git and Maven

== Lab Component Topology

This lab involves quite a few components that support the build, deployment and execution of a rules application running in the Openshift Container Platform.

The component topology and workflow of this lab is as follows:

image::images/dserver1_lab_topology.gif[]

== Lab Set-Up

=== OpenShift

==== Register

In this section of the lab, you register for the ability to create a project in your remote OpenShift lab environment.

. In your browser, navigate to `https://labs.opentlc.com`:
+
image::images/cf_login.png[]

. Enter your OPENTLC username and password and click *Login*.
. Navigate to *Services -> Catalogs -> OPENTLC Cloud Infrastructure Labs -> OpenShift Foundations 3.3 - Beta - NA*:
+
image::images/cf_course_selection_project.png[]


. Click *Order* in the panel on the right.

. In the next panel, click *Submit* in the lower right corner.
+
image::images/cf_submit_button.png[]
+
* Expect to receive a confirmation email that your new OSE3 environment is created.

==== Create a _Project_

. At the command line of your workstation (pre-installed with the _oc_ utility), log into the OpenShift environment:
+
----
$ oc login https://master.na.openshift.opentlc.com
----
+
You will be prompted for your Red Hat GPTE (OPENTLC) SSO credentials.

. Create a new project:
+
-----
$ oc new-project <your initials>-bxmsinfra
-----
+
Going forward, all CLI commands will target this project.

. Make sure you can access your project using the Openshift Web Console.
Open a browser, and navigate to the master node of OpenShift: _https://master.na.openshift.opentlc.com_.
+
Login using the same credentials as used for the CLI.
+
image::images/ocp_home_page.png[]

=== Lab Assets

In a previous lab, you should have already cloned the lab assets for this course to your virtual machine.
If not, execute the following:

. In a terminal window, change directory to the _lab_ directory of your virtual machine.
. Execute:
+
-----
$ git clone https://github.com/gpe-mw-training/bxms-advanced-infrastructure-lab.git
-----

== Gogs Git Server

=== Install

For all _xPaas_ labs in this course, we need a Git server where we will host the code that will be built and deployed on the xPaas Decision Server and Process Server. For this we will use _Gogs_, a Github-like Git server, written in Go (https://gogs.io/).

. In the virtual machine, open a terminal, and change to the directory in the cloned lab project that contains the common templates for the XPaaS labs.
+
----
$ cd /home/jboss/lab/bxms-advanced-infrastructure-lab/xpaas/common
----
. Review the `xpaas-gogs-persistent.yaml` template. This is a template for an installation of Gogs backed by a PostgreSQL database. +
The template defines:
* A Service for the Gogs server and the PostgreSQL server.
* A Route for the Gogs server.
* An ImageStream for the Gogs image. This image is hosted on DockerHub.
* A DeploymentConfig for the Gogs pod.
* A DeploymentConfig for the PostgreSQL pod. The data directory of PostgreSQL is mounted as a volume.
* A PersistentVolumeClaim for the Gogs volume
* A PersistentVolumeClaim for the PostgreSQL volume.
* Parameters:
** *APPLICATION_NAME* : the name for the application, defaults to `gogs`.
** *POSTGRESQL_USER* : the name of the user for the PostgreSQL database, generated.
** *GOGS_POSTGRESQL_PASSWORD* : the password of the user for the PostgreSQL database, generated.
** *VOLUME_CAPACITY* : the volume capacity for the PersistentVolumeClaim, defaults to 512 Mi.

. Create an application based on the template. Specify values for the parameters, if you don't want to use the defaults:
+
----
$ oc process -f xpaas-gogs-persistent.yaml -v APPLICATION_NAME=gogs,POSTGRESQL_USER=gogs,GOGS_POSTGRESQL_PASSWORD=gogs,VOLUME_CAPACITY=512Mi | oc create -f -
----

. Wait a few minutes for the _gogs_ and _postgreslq-gogs_ containers to build and deploy.  Soon enough, you should see only these two containers with a status of _Running_:
+
-----
$ oc get pods
NAME                      READY   STATUS    RESTARTS   AGE
gogs-1-89oy3              1/1     Running   0          3m
postgresql-gogs-1-ctngm   1/1     Running   0          4m
-----

=== Understand the code
Research and attempt to answer the following questions:

. What is the full URL to where the _Gogs_ image used in this lab is hosted? How did you determine that and what information does the homepage of the _Gogs_ image provide?
. What is the port exposed by the service to the postgresql container that the _Gogs_ application connects to ?

ifdef::showscript[]

1)https://hub.docker.com/r/openshiftdemos/gogs/
  - ImageStream of DockerImage is:  openshiftdemos/gogs:latest    ..... which implies Dockerhub.
  - URL provides link to source code of gogs image used for OCP
2)  5432

endif::showscript[]

=== Set-up
Once all OpenShift resources are up, we need to setup the Gogs server.

The Gogs configurations are stored in a file within the running container at:  /etc/gogs/conf/app.ini .

We'll make the initial configuration changes (via a web UI).
We'll then ensure that those changes are made permanent such that if/when a new _gogs_ container replaces this existing one, those config changes continue to be utilized.

==== Modify entries in /etc/gogs/conf/app.ini

. Determine the URL of your _Gogs_ server:
+
-----
$ oc get route
-----
.  Open a browser, and navigate to the URL of the gogs route. +
You should be greeted by the Gogs installation screen:
+
image::images/gogs-installation-screen.png[]
. Fill in the following values:
* *Database type* : PostgreSQL
* *Database Host :* postgresql-gogs:5432
* *Database user:* gogs
* *Database password:* gogs
* *Database name :* gogs
* *SSL Mode:* disable
* *Application Name*: Gogs: Go Git Service
* *Application URL:* http://<gogs route>
* Leave all other settings as is
. Click the button:  `Install Gogs`. +
You are redirected to the Sign in screen. +
Leave the browser window open for now.

. Find the name of the Gogs pod:
+
----
$ gogspod=$(oc get pod | grep "^gogs" | awk '{print $1}')
----
. Review the changes made to the _gogs_ configuration file in the existing container:
+
-----
$  oc exec $gogspod -- cat /etc/gogs/conf/app.ini | more

...

ROOT_URL = http://gogs-bxmsadvdserver.cloudapps.test-ml.opentlc.com/

...

DB_TYPE  = postgres
HOST     = postgresql-gogs:5432
NAME     = gogs
USER     = gogs
PASSWD   = gogs

...

-----

==== Make _gogs_ config changes permanent

As a next step, we can extract this configuration file from the Gogs pod, and mount it as a ConfigMap in the container to make it persistent.

. Create a local file with the contents of the `/etc/gogs/conf/app.ini` file:
+
----
$ oc exec $gogspod -- cat /etc/gogs/conf/app.ini > /tmp/gogs-app.ini
----
. We need to configure Gogs to be able to work with the default self-signed OpenShift certificates. Execute the following command:
+
----
$ sed -i 's/SKIP_TLS_VERIFY = false/SKIP_TLS_VERIFY = true/g' /tmp/gogs-app.ini
----
. Create a ConfigMap from the saved file:
+
----
$ oc create configmap gogs --from-file=/tmp/gogs-app.ini
----
. Mount the configmap as a volume in the Gogs pod:
+
----
$ oc set volume dc/gogs --add --overwrite --name=config-volume -m /etc/gogs/conf/ --source='{"configMap":{"name":"gogs","items":[{"key":"gogs-app.ini","path":"app.ini"}]}}'
----
+
NOTE: This will cause a redeployment of the Gogs pod.

. Wait until the _gogs_ pod has been re-created and is in a status of:  _RUNNING_.
. Create an account and a repository on the Gogs server.
.. Return back to the Gogs login page in your browser.
.. Click on the `Register` link.
+
image::images/gogs_register.png[]

. Create an account. Remember the username and password combination.
. Log in with your username/password combination.
. Create an organization named 	`decision-server-s2i`.
.. Click on the `+` symbol in the upper right, and select `New Organization`.
+
image::images/gogs_new_org.png[]
.. Fill in the name, and click the `Create Organization` button. +
.. Check that you are a member of the new organization. You should be listed as `owner`.
... At the dashboard of the _decision-server-s2i_, click the blue box at the far right: _View decision-server-s2i_
+
image::images/view_dserver.png[]
... Click the `Owners` link.
+
Notice that your userId should be affiliated with this _owners_ group.

. Create a repository in the `decision-server-s2i` organization the with name `policyquote`.
.. Click on the `+` symbol in the upper right, and select: `New Repository`.
.. Make sure the repository is not private. (so don't check that selection box)
.. Make sure the checkbox `Initialize this repository with selected file and template` is unchecked.  +
+
image::images/create_new_repo.png[]
+
Click `Create repository`.
+
Later in the lab we will push our BRMS project to this repository.

== Nexus maven repository server

The S2I build of the Decision Server relies heavily on maven to build and deploy the BRMS project source code. To avoid having to download the maven dependencies at every build cycle, we can configure a Nexus repository as a proxy. The maven build will download the dependencies it needs from the
Nexus proxy rather than the internet, which will drastically improve the build speed.

In this section we will install and configure a Nexus server in our OpenShift project.

=== Install
. In the virtual machine, open a terminal, and change to the directory in the cloned lab project that contains the common templates for the XPaaS labs.
+
----
$ cd /home/jboss/lab/bxms-advanced-infrastructure-lab/xpaas/common
----
. Review the `xpaas-nexus-persistent.yaml` template. This is a template for the installation of Nexus. +
The template defines:
* A Service for the Nexus server.
* A Route for the Nexus server.
* An ImageStream for the Nexus docker image. This image is hosted on DockerHub.
* A DeploymentConfig for the Nexus pod.
* A PersistentVolumeClaim for the Nexus volume, to hold the Nexus configuration and storage.
* Parameters:
** APPLICATION_NAME : the name for the application, defaults to `nexus`.
** VOLUME_CAPACITY : the volume capacity for the PersistentVolumeClaim, defaults to 512 Mi.

. Create an application based on the template. Specify values for the parameters, if you don't want to use the defaults:
+
----
$ oc process -f xpaas-nexus-persistent.yaml -v APPLICATION_NAME=nexus,VOLUME_CAPACITY=512Mi | oc create -f -
----

=== Configure
Once all components of our application are up, configure the Nexus server.
More specifically, we need to add the Red Hat enterprise maven repository to the list of proxied repositories.

. In a browser window, navigate to the URL of the Nexus route.
. Log in with the `admin/admin123` username/password.
. Click on the `Repositories` on the left menu.
. Click on the `Add...` icon in the top menu.  Doing so provides a drop-down of options.
. From the selection drop dwon, choose to create a: `Proxy Repository`
. In the `New Proxy Repository` form, fill in the following values:
** Repository ID: redhat-ga
** Repository Name: Red Hat GA
** Remote Storage Location : https://maven.repository.redhat.com/ga/
** Leave the other fields as is.
** Click `Save`
. Add the _Red Hat GA repository_ to the public repository group.
+
image::images/nexus-redhat-repo.png[]
.. Click on the `Repositories` on the left menu, and then on the `Public Repositories` in the list of repositories.
.. In the bottom pane, click on the `Configuration` tab.
.. Make sure that the `Red Hat GA` repository is in the `Ordered Group Repositories` pane.
.. Click `Save`.

== Policyquote sample application

The application used in this lab is called:  _Policyquote_.

The _Policyquote_ application is a fairly simple BRMS application to calculate the price of a car insurance policy based on driver and car data. The project consists of a number of rules (including a ruleflow process), and a domain model in a single maven project.

[NOTE]
The S2I build mechanism imposes certain limitations on the project structure. Multi-module maven projects are not well supported. Specifically for kjars, all dependencies (like a domain model jar) should be available in a maven repository before the build kicks off. +
When using binary deployments, you have more flexibility on how to structure your project.

In this part of the lab, we will clone the Policyquote project from Github, and push it into our Gogs server on OpenShift to act as source for our S2I builds.

. In the virtual machine, open a terminal and change to the lab home folder.
+
----
$ cd /home/jboss/lab
----
. Clone the Policyquote project from the GPTE Github site:
+
----
$ git clone https://github.com/gpe-mw-training/bxms-xpaas-policyquote
----
. Add a remote repository to the cloned project pointing to our Gogs git server:
+
----
$ cd bxms-xpaas-policyquote
$ git remote add gogs-s2i http://<gogs username>:<gogs password>@<url of the gogs route>/decision-server-s2i/policyquote.git
----
+
Replace `<gogs password>`,`<url of the gogs route>` and `<gogs username>` with the appropriate values for your environment.
. Push the code to the Gogs server:
+
----
$ git push gogs-s2i master
----
. Using your browser, return to the home page of your `decision-server-s2i` repository hosted in your _gogs_ container
+
image::images/seeded_gogs_repo.png[]
. Notice that your repo is now seeded with the _policyquote_ project.
. Review the code and rules found in this application.
.. Notice that the project includes a drools _ruleflow_ artifact:  _PolicyQuote.rf_
+
If you were to view this ruleflow file in JBoss Developer Studio (assuming JBDS is installed with the _Integration Stack_ of plugins), you'd see that the ruleflow is as follows:
+
image::images/policy-quote-rule-flow.png[]
.. Study each of the rule files found in this project.
*** What are the names of the rules affiliated with the _calculation_ ruleflow-group ?
*** What are the names of the rules affiliated with the _surcharge_ ruleflow-group ?


== Decision Server S2I Templates

To create Decision Server applications on OpenShift, we can start from a template that we will import into our OpenShift project. As we can have several templates using the same Decision Server image, we will first create an image stream for the Decision Server image, so that we can reuse the image stream in several templates.

. In the virtual machine, open a terminal, and change to the directory in the cloned lab project that contains the templates for the Decision Server lab.
+
----
$ cd /home/jboss/lab/bxms-advanced-infrastructure-lab/xpaas/decision-server
----
. Review the `decisionserver-63-is.yaml` definition file. This file defines the ImageStream for the Decision Server 6.3 image, hosted in the Red Hat docker registry. The latest version of this image is 1.3.
. Create the ImageStream for the Decision Server image:
+
----
$ oc create -f decisionserver-63-is.yaml
----
. Review the `decisionserver-basic-s2i.yaml` template.
.. This template defines:
* A BuildConfig for the S2I build. +
The BuildConfig defines a source build, pointing to a git repo, as well as the builder image, through the ImageStream we defined earlier. +
The build will be triggered through a webhook (triggered whenever we push new code to the git repository), or by a change in the builder image.
* An ImageStream for the image created as a result of the build.
* A DeploymentConfig for the pod(s) running the image created as result of the build. The number of replica's is set to one.
* A Service for the Decision Server.
* A Route for the Decision Server.
* Parameters:
** *KIE_CONTAINER_DEPLOYMENT :* describes what kjar(s) needs to be deployed on the Decision Server, in the format `containerId=groupId:artifactId:version|c2=g2:a2:v2`
** *KIE_CONTAINER_REDIRECT_ENABLED :* Enable redirect functionality for KIE containers. Defaults to true. Should be true when different versions of the same kjar are to be deployed side-by-side.
** *KIE_SERVER_USER :* the user name to access the KIE Server REST or JMS interface. Defaults to `kieserver`.
** *KIE_SERVER_PASSWORD :* The password to access the KIE Server REST or JMS interface. Defaults to a generated value.
** *APPLICATION_NAME :* the name for the application.
** *HOSTNAME_HTTP :* Custom hostname for the http service route. Leave blank for default hostname generated by OpenShift.
** *SOURCE_REPOSITORY_URL :* Git source URI for application. Required.
** *SOURCE_REPOSITORY_REF :* the Git branch/tag reference to build. Defaults to `master`.
** *CONTEXT_DIR :* The path within the Git project to build. Leave blank for the root project directory.
** *GITHUB_WEBHOOK_SECRET :* GitHub trigger secret. Will be added to the webhook URL. Defaults to a generated value.
** *GENERIC_WEBHOOK_SECRET:* Generic build trigger secret. Will be added to the webhook URL. Defaults to a generated value.
** *IMAGE_STREAM_NAMESPACE :* Namespace in which the ImageStreams for Red Hat xPaaS images are installed. These ImageStreams are normally installed in the openshift namespace. You should only need to modify this if you've installed the ImageStreams in a different namespace/project (which is the case in our lab).
** *MAVEN_MIRROR_URL :* The URL of the maven mirror (Nexus server)

.. A few other considerations related to this template are as follows:
... This template does not contain a database service (Decision Server does not use persistence).
... The Decision Server uses an insecure route (http, no https).

. Import the template into your OpenShift project:
+
----
$ oc create -f decisionserver-basic-s2i.yaml
----

== Openshift Decision Server Application

Everything is in place now to create an OpenShift application from our BRMS project.

. In the virtual machine, open a terminal, and issue the following commands (replace expressions between `<>` with correct values for your environment):
+
----
$ application_name=policyquote-app
$ source_repo=http://gogs:3000/decision-server-s2i/policyquote.git
$ nexus_url=http://nexus:8081
$ kieserver_password=kieserver1!
$ is_namespace=<name of your OpenShift project>
$ kie_container_deployment="policyquote=com.redhat.gpte.xpaas:policyquote:1.0-SNAPSHOT"
$ oc new-app --template=decisionserver63-basic-s2i -p KIE_SERVER_PASSWORD=$kieserver_password,APPLICATION_NAME=$application_name,SOURCE_REPOSITORY_URL=$source_repo,IMAGE_STREAM_NAMESPACE=$is_namespace,KIE_CONTAINER_DEPLOYMENT=$kie_container_deployment,KIE_CONTAINER_REDIRECT_ENABLED=false,MAVEN_MIRROR_URL=$nexus_url/content/groups/public/
----
+
* Note that the KIE_CONTAINER_REDIRECT_ENABLED environment variable is set to false. This means that the name of the KIE-Container for our application will be `policyquote`, as defined in KIE_CONTAINER_DEPLOYMENT.

. Check the progress of the build and deployment of the application in the OpenShift console.
* As this is the first build, it will take quite some time: the builder image needs to be downloaded from the Red Hat docker repository, and the Nexus maven proxy needs to be seeded with the build dependencies.
* The S2I build is happening in a builder pod, named `policyquote-app-1-build`. Check the logs for this pod in the web console, or use the Openshift CLI:
+
----
$ oc logs -f policyquote-app-1-build
----
* At the end of the build cycle, you should see the following in the builder pod log:
+
----
I0908 06:48:48.042137       1 sti.go:334] Successfully built xpaas/policyqote-app-1:a0ec7e20
I0908 06:48:48.118123       1 cleanup.go:23] Removing temporary directory /tmp/s2i-build455291570
I0908 06:48:48.118178       1 fs.go:156] Removing directory '/tmp/s2i-build455291570'
I0908 06:48:48.139557       1 sti.go:268] Using provided push secret for pushing 172.30.1.250:5000/xpaas/policyqote-app:latest image
I0908 06:48:48.139575       1 sti.go:272] Pushing 172.30.1.250:5000/xpaas/policyqote-app:latest image ...
I0908 06:51:52.519695       1 sti.go:288] Successfully pushed 172.30.1.250:5000/xpaas/policyqote-app:latest
----
* The image built by the builder pod is pushed to the OpenShift internal registry.
This will trigger the deployment of the image.
* To check the logs of the application pod, locate the pod (name `policyquote-app-1-xxxxx`), and check the logs in the OpenShift console or with the CLI.
* After some time, you will see something like:
+
----
06:53:27,949 INFO  [org.kie.server.services.impl.KieServerImpl] (EJB default - 1) Container policyquote (for release id com.redhat.gpte.xpaas:policyquote:1.0-SNAPSHOT) successfully started
----
* By that time, the service and the route will be started, and our Decision Server application is ready to serve requests.
+
image::images/policyquote-application-ose.png[]

== View KIE Server status

. Investigate KIE Server API documentation
+
Before we get begin to execute our deployed _policyquote_ application, lets investigate the details of the API exposed by the KIE Server.

.. Determine the route to the _policyquote_ application deployed to a _decision-server_ container in OpenShift:
+
-----
$ oc get route | grep policyquote
-----
.. Using your browser, navigate to: `<policyquote app route>/kie-server/docs`.
.. Pay particular attention to the API that accepts a HTTP POST at the following uri:  `server/containers/instances/{id}`
+
image::images/kie-server-api-post.png[]
+
NOTE: Notice the use of the term _containers_ in the URI of the above resource.
The word _container_ is highly overloaded in the world of software.
The use of the word _container_ in this specific context refers to the Red Hat JBoss BRMS construct: _KIE Container_.
It does not refer to a OpenShift / Kubernetes _container_.

... It is this resource URI that is utilized to drive the stateless rules engine that runs in our _decision server_ docker container.
+
The _id_ specified in the resource URI refers to the identifier of the drools _KIE container_ to invoke;
in our case the KIE container is called:  _policyquote_.
... This REST resource accepts a payload (in either XML or JSON representation) of drools API _batch_ commands_.
... For some perspective, the code equivalent of these _batch commands_ can be found here:
.... link:https://github.com/droolsjbpm/drools/tree/master/drools-core/src/main/java/org/drools/core/command/runtime/rule[rule commands]
.... link:https://github.com/droolsjbpm/drools/tree/master/drools-core/src/main/java/org/drools/core/command/runtime/processhttps://github.com/droolsjbpm/drools/tree/master/drools-core/src/main/java/org/drools/core/command/runtime/process[process commands]

. For the remainder of this lab, we will use the command line based `curl` utility to interact with the RESTful API exposed by our Decision Server application. +
In a terminal window, issue the following commands:
+
----
$ policyquote_app=<URL of the policyquote app route>
$ kieserver_password=kieserver1!
----
. To check the health of the server:
+
----
$ curl -X GET -H "Accept: application/json" --user kieserver:$kieserver_password "$policyquote_app/kie-server/services/rest/server"
----
+
Response:
+
----
{
  "type" : "SUCCESS",
  "msg" : "Kie Server info",
  "result" : {
    "kie-server-info" : {
      "version" : "6.4.0.Final-redhat-3",
      "name" : "kieserver-policyquote-app-1-xlgac",
      "location" : "http://policyquote-app-1-xlgac:8080/kie-server/services/rest/server",
      "capabilities" : [ "BRM", "KieServer" ],
      "messages" : [ {
        "severity" : "INFO",
        "timestamp" : 1473333794748,
        "content" : [ "Server KieServerInfo{serverId='kieserver-policyquote-app-1-xlgac', version='6.4.0.Final-redhat-3', location='http://policyquote-app-1-xlgac:8080/kie-server/services/rest/server'}started successfully at Thu Sep 08 07:23:14 EDT 2016" ]
      } ],
      "id" : "kieserver-policyquote-app-1-xlgac"
    }
  }
}
----
. To check which KIE-Containers are deployed on the server:
+
----
$ curl -X GET -H "Accept: application/json" --user kieserver:$kieserver_password "$policyquote_app/kie-server/services/rest/server/containers"
----
Response:
+
----
{
  "type" : "SUCCESS",
  "msg" : "List of created containers",
  "result" : {
    "kie-containers" : {
      "kie-container" : [ {
        "status" : "STARTED",
        "messages" : [ {
          "severity" : "INFO",
          "timestamp" : 1473333804577,
          "content" : [ "Container policyquote successfully created with module com.redhat.gpte.xpaas:policyquote:1.0-SNAPSHOT." ]
        } ],
        "container-id" : "policyquote",
        "release-id" : {
          "version" : "1.0-SNAPSHOT",
          "group-id" : "com.redhat.gpte.xpaas",
          "artifact-id" : "policyquote"
        },
        "resolved-release-id" : {
          "version" : "1.0-SNAPSHOT",
          "group-id" : "com.redhat.gpte.xpaas",
          "artifact-id" : "policyquote"
        },
        "config-items" : [ ]
      } ]
    }
  }
}
----

=== Test _policyquote_ application
. To test our application, we need to send a correctly formatted payload.
The `xpaas/decision-server` directory of the lab contains an example, formatted as JSON.
.. Open the json payload file _policyquote-payload.json_ and study its contents.
+
Notice how the various batch commands found in this json payload file correspond to similar java _Command_ objects found in the _rule_ and _process_ directories of the link:https://github.com/droolsjbpm/drools/tree/master/drools-core/src/main/java/org/drools/core/command/runtime[Drools command source code].
.. Make sure you are in the `xpaas/decision-server` directory, and execute:
+
----
$ curl -s -X POST -H "Content-Type: application/json" -H "Accept: application/json" --user kieserver:$kieserver_password -d @policyquote-payload.json "$policyquote_app/kie-server/services/rest/server/containers/instances/policyquote"
----
+
Response:
+
----
{
  "type": "SUCCESS",
  "msg": "Container policyquote successfully called.",
  "result": {
    "execution-results": {
      "results": [
        {
          "key": "driver",
          "value": {
            "com.redhat.gpte.policyquote.model.Driver": {
              "id": "1",
              "driverName": "John Doe",
              "age": 26,
              "ssn": "789456",
              "dlNumber": "123456",
              "numberOfAccidents": 2,
              "numberOfTickets": 1,
              "creditScore": 0
            }
          }
        },
        {
          "key": "policy",
          "value": {
            "com.redhat.gpte.policyquote.model.Policy": {
              "requestDate": null,
              "policyType": "AUTO",
              "vehicleYear": 1999,
              "price": 300,
              "priceDiscount": 0,
              "driver": "1"
            }
          }
        }
      ],
      "facts": [
        {
          "key": "driver",
          "value": {
            "org.drools.core.common.DefaultFactHandle": {
              "external-form": "0:1:725414105:725414105:1:DEFAULT:NON_TRAIT:com.redhat.gpte.policyquote.model.Driver"
            }
          }
        },
        {
          "key": "policy",
          "value": {
            "org.drools.core.common.DefaultFactHandle": {
              "external-form": "0:2:1271576022:1271576022:3:DEFAULT:NON_TRAIT:com.redhat.gpte.policyquote.model.Policy"
            }
          }
        }
      ]
    }
  }
}
----
+
Of particular importance in the response is the price field of the Policy, which has been set as a result of the execution of the rules in our application. +
To filter out the price field, use `grep`:
+
----
$ curl -s -X POST -H "Content-Type: application/json" -H "Accept: application/json" --user kieserver:$kieserver_password -d @policyquote-payload.json "$policyquote_app/kie-server/services/rest/server/containers/instances/policyquote" | grep '"price"'
----
+
----
  "price" : 300,
----
. Feel free to change some values in the payload file (`policyquote-payload.json`) for the Driver and Policy objects, and check if you get another result from the server. You can review the rules in the project to have an idea what fields need to be changed to influence the calculated price.

== Application lifecycle

Now we can introduce a change in one of the rules of our application, and observe what's happening when we push the change to the git repository. +

=== Configure WebHook to OCP BuildConfig object
First we need to define a webhook in our policyquote repository on Gogs, that will be triggered by a push of new code. The webhook calls the Openshift API in order to start a new S2I build.

. In a terminal window, issue the following command:
+
----
oc describe bc policyquote-app
----
+
From the response, copy the URL of the GitHub Webhook. This should look like:
+
----
https://<OpenShift URL>:8443/oapi/v1/namespaces/xpaas/buildconfigs/policyquote-app/webhooks/<secret>/github
----
. Open a browser window and navigate to the policyquote repository on Gogs. Click on the `Settings` link in the top right.
+
image::images/gogs-repository-settings.png[]
. In the settings window menu, click on `Webhooks`, and then on `Add Webhook`. Choose the `Gogs` format.
. Paste the webhook URL obtained from the from the BuildConfig into the `Payload URL` text box. +
Leave `Content Type` to application/json, and leave `Secret` blank. +
Make sure the `Just the push event` radio button and the `Active` check box are selected. +
Click `Add Webhook`.

=== Test Change to Rules
. In a terminal window, change to the root of the cloned `bxms-xpaas-policyquote` project.
. Open the `src/main/resources/RiskyAdults.drl` file for editing. Change the price in the rule action to 350. +
The rule should now look like:
+
----
package com.redhat.gpte.policyquote;

import com.redhat.gpte.policyquote.model.Driver
import com.redhat.gpte.policyquote.model.Policy

rule "RiskyAdults"

    ruleflow-group "calculation"

    when
        //conditions
        $driver : Driver(age > 24, numberOfAccidents >= 1 || numberOfTickets >=2, $id : id)
        $policy : Policy(price == 0, policyType == "AUTO", driver == $id)
    then
        //actions
        modify($policy) {setPrice(350)};

end
----
. As the project contains some unit tests for our rules, (like it should be, right?), we need to make a change there as well. +
Open the `src/test/java/com/redhat/gpte/policyquote/rules/RiskyAdultsTest.java` for editing. Change the assert around line 62 to:
+
----
Assert.assertEquals(350, policy.getPrice().intValue());
----
. Optionally, you can test if the project builds sucessfully by doing a local maven build:
+
----
$ mvn clean package
----
. If the build succeeds, push the changes to the Gogs git server:
+
----
$ git add --all
$ cat << EOF > ~/.gitconfig
[user]
email = gptestudent@gptestudent.com
name = gptestudent
EOF
$ git commit -m "raised the price for risky adults"
$ git push gogs-s2i master
----
. Check in the Openshfift web console that a new build is triggered by the code push.
+
image::images/openshift-s2i-new-build.png[]
+
Note that this build does not take as long as the first one.
. Once the new build is completed, the original application pod is teared down, while the new build pod is being deployed.
+
image::images/openshift-s2i-new-deployment.png[]
. Test the new deployment.
.. Change the directory to: `~/lab/bxms-advanced-infrastructure-lab/xpaas/decision-server`
.. Execute:
+
----
curl -s -X POST -H "Content-Type: application/json" -H "Accept: application/json" --user kieserver:$kieserver_password -d @policyquote-payload.json "$policyquote_app/kie-server/services/rest/server/containers/instances/policyquote" | grep '"price"'
----
+
----
  "price" : 350,
----
.. The price should now be 350 instead of 300.

=== Scaling out and Rolling Deployment

As you will have noticed during the build and deployment triggered by a code change, there is a time span during which the application is unavailable. This happens grosso modo between the moment that the S2I build is finished, and the new deployment is active. This includes the time needed by the Decision Server to start up. +
In a development phase, this is not so dramatic, but it is probably not acceptable in a production environment.
By scaling out our application, and defining a *rolling upgrade strategy*, we can ensure that our application remains available, even if that means that during a limited time span both the old as the new version will be deployed concurrently.

We are going to introduce the changes required directly in the DeploymentConfig of our application. Alternatively, you could create the changes in the template, load it into the OpenShift project, tear down the existing application and create a new one based on the modified template.

. In a terminal window, execute the following command:
+
----
$ oc edit dc policyquote-app
----
+
This will open the DeploymentConfig definition in YAML format in vi. +
If you are unfamiliar with vi, you can also edit the DeploymentConfig directly in the OpenShift web console. Navigate to the policyquote deployment, click on the `Actions` button in the top left, and choose `Edit YAML`. This will open a popup window in which you can edit the YAML file.
. Change the `spec/replicas` and the `spec/strategy` section to match the following content. Note that YAML is indentation sensitive.
+
----
spec:
  replicas: 2
[...]
  strategy:
    recreateParams:
      timeoutSeconds: 600
    resources: {}
    rollingParams:
      maxSurge: 1
      maxUnavailable: 1
      timeoutSeconds: 600
    type: Rolling
[...]
----
+
We raised the number of required pods for our application to 2, and defined a Rolling deployment strategy. During deployment, at most one pod will be made unavailable (maxUnavailable), and we will create at most one extra pod on top of the replica count (maxSurge).
. Save the file. As a result, a new policy quote application pod will be deployed, bringing the number of pods to 2.
+
image::images/policyquote-deployment-scaled.png[]
+
Requests to the application will now be balanced between the two pods. You can use curl to test that our application is still working fine.
. Repeat the instructions detailed above to make a change in the code of the application. +
This time, change the price in the Risky Adult rule to 400. Don't forget to change the unit test accordingly. Build locally, commit and push the change.
. To monitor the availability of the application, use the curl command in a loop.
+
----
$ while [ true ]; do curl -s -X POST -H "Content-Type: application/json" -H "Accept: application/json" --user kieserver:$kieserver_password -d @policyquote-payload.json "$policyquote_app/kie-server/services/rest/server/containers/instances/policyquote" | grep '"price"'; sleep 2; done
----
. When the build is finished, the rolling deployment will start deploying the new application pods, but as long as at least one of the new pods is not active, the old pod will not be teared down.
+
image::images/policyquote-deployment-rolling.png[]
+
If you launched the curl command in a loop you should haved noticed no interruption in the responsiveness of the application. When the new application pods become active, the application responds with a price of 400 rather than 350.

== Cleanup
This concludes the first lab of this module.
To save resources on Openshift, you can tear down the policyquote application.
Leave the Nexus and Gogs applications running, as we will need them for the next lab.

In a terminal window, issue the following commands:

----
$ oc delete dc policyquote-app
$ oc delete service policyquote-app
$ oc delete route policyquote-app
$ oc delete is policyquote-app
$ oc delete bc policyquote-app
$ for pod in `oc get pod | grep "\-build" | awk '{print $1}'`; do oc delete pod $pod; done
----

ifdef::showscript[]

nice job with decision server lab.  still going through it.  really like the use of the ConfigMap object for the gogs server
i think it would be valuable to:￼
1)  point out to students that there are existing decisionserver templates in the openshift namespace
2)  our rationale for not leveraging those templates directly as is.  Sounds like one reason is the desire to isolate and re-use the decision server image stream (edited)

Actually there are a couple of reasons to use our own template and image stream:
* The imagestream and templates are not installed by default on OCP < 3.3 (at least not the latest versions)
* The templates in the openshift namespace miss the MAVEN_MIRROR parameter, which makes it a lot harder to leverage nexus as a maven proxy
* The templates in the openshift namespace have a lot of parameters (especially the process server templates) that are not required and might confuse students
* In general I think in real life most people will come up with templates customized to their needs, rather than using the provided ones.
These should be more considered as examples or blueprints.


2)  study and elaborate on:
  - KIE_CONTAINER_DEPLOYMENTKIE_CONTAINER_DEPLOYMENT
  - KIE_CONTAINER_REDIRECT_ENABLED

endif::showscript[]
