:scrollbar:
:data-uri:
:toc2:
:ocdownload: link:https://access.redhat.com/downloads/content/290/ver=3.5/rhel---7/3.5.5.31/x86_64/product-software[OpenShift CLI client]


== Decision Server S2I Deployment Lab


This module covers the Realtime Decision Server feature of JBoss BRMS for OpenShift. The Decision Server runtime is configured for deployment on OpenShift Container Platform. The Decision Server supports source-to-image (S2I) builds, as well as binary deployments starting from prebuilt kjars.

In this lab, you explore the S2I functionality of the Decision Server.

.Goals

* Provision the Decision Server in an OpenShift environment
* Deploy a rules-based `kie-project` to the Decision Server
* Automate the build and redeployment processes when code changes are made to your `kie-project`
* Investigate various OpenShift deployment strategies to reduce downtime

.Requirements

* Completion of the _Application Development with Red Hat OpenShift_ course offered by Red Hat OPEN and SkillsExchange
* Familiarity with the `kie-server` API of the JBoss BPM Suite product
* Experience using Git and Maven
* Access to GitHub to clone the projects used in this lab

.Lab Environment
Most of the software utilized in this lab (along with the next lab focused on the OCP _process server_) executes in a remote OCP environment.
There is some client-side tooling needed for these labs.  In particular:

* {ocdownload} (`oc`)
* Git
* Maven 3.3.* (or more recent)
* curl

NOTE: For those using the virtual machine for this course, the above listed tooling is already installed.
For those not using the virtual machine for this course, please install the above listed tooling at this time.

== Lab Component Topology

This lab involves quite a few components that support the build, deployment, and execution of a rules application running in OpenShift Container Platform.

The following illustrates the component topology and workflow of this lab:

image::images/dserver1_lab_topology.gif[]



:numbered:

== Set Up the OpenShift Lab Environment

=== Register

In this section of the lab, you register for the ability to create a project in your remote OpenShift lab environment.

. In your browser, navigate to the link:https://labs.opentlc.com/[OPENTLC lab portal]:
+
image::images/cf_login.png[]

. Enter your OPENTLC username and password and click *Login*.
. Navigate to *Services -> Catalogs -> OPENTLC Middleware Solutions Labs -> BXMS_Infra*:
+
image::images/cf_course_selection_project.png[]

. Click *Order* in the panel on the right.

. In the next panel, click *Submit* in the lower right corner.
+
image::images/cf_submit_button.png[]
+
. Expect to receive an email with contents similar to the following:
+
-----
You have been enrolled in the OPENTLC BXMS_INFRA OpenShift environment.
You can access your environment by going to https://master.na.openshift.opentlc.com:443 and logging in as jbride-test.

NOTICE: Your environment will expire and be deleted in 14 day(s) at 2017-02-07 00:00:00 -0500. In order to conserve resources we cannot archive or restore any data in this environment. All data will be lost upon expiration.
-----
+
This email confirms that your OpenShift project was created.
Please save this email.
+
Pay particular attention to the URL of the OCP master node referenced in this email.
You will be navigating to this OCP master node for the duration of this course.

=== Use your OCP Project

. At the command line of your workstation (pre-installed with the _oc_ utility), log in to the OpenShift environment:
+
[source,text]
----
$ oc login <URL of OCP Master node listed in the confirmation email>
----

* You will be prompted for your OPENTLC SSO credentials.

. Notice that an OCP project will have already been created for you :
+
-----
$ oc project
Using project "jbride-test-bxms-infra" on server "https://xxx.xxx.openshift.opentlc.com:443".
-----
+
Going forward, all CLI commands will target this project.

. Make sure you can access your project using the OpenShift web console:
.. Open a browser.
.. Navigate to the master node of OpenShift (URL was provided to you in the confirmation email when registering for the _BXMS_INFRA_ OCP lab environment

.. Log in using the same credentials you used for the CLI.
+
image::images/ocp_home_page.png[]

=== Clone the Lab Assets

If you did not clone the lab assets for this course to your VM in a previous lab, you must do so now:

. In a terminal window, change to the `lab` directory of your VM.
. Run the following:
+
[source,text]
-----
$ git clone https://github.com/gpe-mw-training/bxms-advanced-infrastructure-lab.git
-----

== Configure the Gogs Git Server

=== Install the Server

For all xPaaS labs in this course, you need a Git server to host the code that you build and deploy on the Decision Server and Process Server. For this, you use Gogs, a GitHub-like Git server written in Go (https://gogs.io/).

. In the VM:
.. Open a terminal.
.. Change to the directory in the cloned lab project that contains the common templates for the XPaaS labs:
+
[source,text]
----
$ cd /home/jboss/lab/bxms-advanced-infrastructure-lab/xpaas/common
----
. Review the `xpaas-gogs-persistent.yaml` template. This is a template for an installation of Gogs backed by a PostgreSQL database. The template defines:
* A Service for the Gogs server and the PostgreSQL server
* A Route for the Gogs server
* An ImageStream for the Gogs image. This image is hosted on DockerHub
* A DeploymentConfig for the Gogs pod
* A DeploymentConfig for the PostgreSQL pod. The data directory of PostgreSQL is mounted as a volume
* A PersistentVolumeClaim for the Gogs volume
* A PersistentVolumeClaim for the PostgreSQL volume
* Parameters:
** *APPLICATION_NAME:* The name for the application (default is `gogs`)
** *POSTGRESQL_USER:* The generated name of the user for the PostgreSQL database
** *GOGS_POSTGRESQL_PASSWORD:* The generated password of the user for the PostgreSQL database
** *VOLUME_CAPACITY:* The volume capacity for the PersistentVolumeClaim, in MB (default is `512`)

. Create an application based on the template. Specify values for the parameters if you do not want to use the defaults:
+
[source,text]
----
$ oc process -f xpaas-gogs-persistent.yaml -v APPLICATION_NAME=gogs,POSTGRESQL_USER=gogs,GOGS_POSTGRESQL_PASSWORD=gogs,VOLUME_CAPACITY=512Mi | oc create -f -
----

. Wait a few minutes for the `gogs` and `postgreslq-gogs` containers to build and deploy. After that time, only these two containers should display a status of `Running`:
+
[source,text]
-----
$ oc get pods
NAME                      READY   STATUS    RESTARTS   AGE
gogs-1-89oy3              1/1     Running   0          3m
postgresql-gogs-1-ctngm   1/1     Running   0          4m
-----

=== Understand the Code

Research and attempt to answer the following questions:

* What is the full URL that indicates where the Gogs image used in this lab is hosted?
** How did you determine this URL?
** What information does the home page of the Gogs image provide?
* What is the port exposed by the service to the `postgresql` container to which the Gogs application connects?

ifdef::showscript[]

1) https://hub.docker.com/r/openshiftdemos/gogs/
  - ImageStream of DockerImage is:  openshiftdemos/gogs:latest    ..... which implies Dockerhub.
  - URL provides link to source code of gogs image used for OCP
2)  5432

endif::showscript[]

=== Set Up the Server: Modify Entries in `/etc/gogs/conf/app.ini`

After you have set up all of the OpenShift resources, you need to set up the Gogs server.

The Gogs configurations are stored in a file within the running container at  `/etc/gogs/conf/app.ini`.

First you make the initial configuration changes via a web UI.

. Determine the URL of your Gogs server:
+
[source,text]
-----
$ oc get route
-----
.  Open a browser and navigate to the `gogs` route URL.
+
The Gogs installation screen appears:
+
image::images/gogs-installation-screen.png[]

. Enter the following values:
* *Database type:* `PostgreSQL`
* *Database Host:* `postgresql-gogs:5432`
* *Database user:* `gogs`
* *Database password:* `gogs`
* *Database name:* `gogs`
* *SSL Mode:* `disable`
* *Application Name*: `Gogs: Go Git Service`
* *Application URL:* `http://<gogs route>`
+
* Leave all other settings as is.

. Click the *Install Gogs* button.
* You are redirected to the *Sign in* screen. Leave the browser window open for now.

. Find the name of the Gogs pod:
+
[source,text]
----
$ gogspod=$(oc get pod | grep "^gogs" | awk '{print $1}')
----
. Review the changes made to the `gogs` configuration file in the existing container:
+
[source,text]
-----
$  oc exec $gogspod -- cat /etc/gogs/conf/app.ini | more

...

ROOT_URL = http://gogs-bxmsadvdserver.cloudapps.test-ml.opentlc.com/

...

DB_TYPE  = postgres
HOST     = postgresql-gogs:5432
NAME     = gogs
USER     = gogs
PASSWD   = gogs

...

-----

=== Set Up the Server: Make `gogs` Configuration Changes Permanent

Next you ensure that the changes you made are permanent, so that any new `gogs` container replacing the existing one continues to use your configuration changes.

To do so, you extract the configuration file from the Gogs pod and mount it as a ConfigMap in the container to make it persistent.

. Create a local file with the contents of the `/etc/gogs/conf/app.ini` file:
+
[source,text]
----
$ oc exec $gogspod -- cat /etc/gogs/conf/app.ini > /tmp/gogs-app.ini
----
. Configure Gogs to work with the default self-signed OpenShift certificates:
+
[source,text]
----
$ sed -i 's/SKIP_TLS_VERIFY = false/SKIP_TLS_VERIFY = true/g' /tmp/gogs-app.ini
----
. Create a ConfigMap from the saved file:
+
[source,text]
----
$ oc create configmap gogs --from-file=/tmp/gogs-app.ini
----
. Mount the ConfigMap as a volume in the Gogs pod:
+
[source,text]
----
$ oc set volume dc/gogs --add --overwrite --name=config-volume -m /etc/gogs/conf/ --source='{"configMap":{"name":"gogs","items":[{"key":"gogs-app.ini","path":"app.ini"}]}}'
----
* This causes a redeployment of the Gogs pod.

. Wait until the `gogs` pod has been re-created and has a `RUNNING` status.

. Create an account and a repository on the Gogs server:
.. Go back to the Gogs login page in your browser.
.. Click *Register*.
+
image::images/gogs_register.png[]
.. Create an account. Be sure to remember the username and password combination.
.. Log in with your username and password.


=== Configure `decision-server-s2i` organization in Gogs

. Create an organization named `decision-server-s2i`:
.. Click *+* at the upper right.
.. Click *New Organization*.
+
image::images/gogs_new_org.png[]
.. For the organization name, enter `decision-server-s2i`.
.. Click the *Create Organization* button.
.. Check that you are a member of the new organization and listed as `owner`.
.. From the dashboard of the `decision-server-s2i`, click the blue box at the far right: *View decision-server-s2i*.
+
image::images/view_dserver.png[]
.. Click the *Owners* link and confirm that your userId is affiliated with this Owners group.

. Create a `policyquote` repository in the `decision-server-s2i` organization:
.. Click the `+` symbol in the upper right, and select *New Repository*.
.. Make sure the repository is not private. (Do not check that box.)
.. Make sure the *Initialize this repository with selected file and template* box is unchecked.
+
image::images/create_new_repo.png[]
+
.. Click *Create repository*.
+
* Later in the lab, you push your BRMS project to this repository.

== Configure the Nexus Maven Repository Server

The S2I build of the Decision Server relies heavily on Maven to build and deploy the BRMS project source code. To avoid needing to download the Maven dependencies at every build cycle, you can configure a Nexus repository as a proxy. The Maven build downloads the dependencies it needs from the Nexus proxy rather than the internet, which speeds up the build considerably.

In this section, you install and configure a Nexus server in your OpenShift project.

=== Install

. In the VM, open a terminal and change to the directory in the cloned lab project that contains the common templates for the xPaaS labs:
+
[source,text]
----
$ cd /home/jboss/lab/bxms-advanced-infrastructure-lab/xpaas/common
----

. Review the `xpaas-nexus-persistent.yaml` template. This is a template for the installation of Nexus. The template defines:
* A Service for the Nexus server
* A Route for the Nexus server
* An ImageStream for the Nexus docker image, which is hosted on DockerHub
* A DeploymentConfig for the Nexus pod
* A PersistentVolumeClaim for the Nexus volume, which holds the Nexus configuration and storage
* Parameters:
** *APPLICATION_NAME:* The name for the application (default is `nexus`)
** *VOLUME_CAPACITY:* The volume capacity for the PersistentVolumeClaim, in MB (default is `512`)

. Create an application based on the template. Specify values for the parameters if you do not want to use the defaults:
+
[source,text]
----
$ oc process -f xpaas-nexus-persistent.yaml -v APPLICATION_NAME=nexus,VOLUME_CAPACITY=512Mi | oc create -f -
----

=== Configure

Once all the components of your application are up, you are ready to configure the Nexus server.
More specifically, you need to add the Red Hat enterprise Maven repository to the list of proxied repositories.

. In a browser window, navigate to the URL of the Nexus route.
. Log in with the username `admin` and password `admin123`.
. In the left navigation panel, click *Repositories*.
.. Click the *Add* icon in the top menu to access the list of options.
.. Click  *Proxy Repository*.
.. On the *New Proxy Repository* screen, enter the following values:
* *Repository ID:* `redhat-ga`
* *Repository Name:* `Red Hat GA`
* *Remote Storage Location:* `https://maven.repository.redhat.com/ga/`
+
* Leave the other fields as is.
.. Click *Save*.

. Add the Red Hat GA repository to the public repository group:
.. In the left navigation panel, click *Repositories*.
.. Select *Public Repositories*.
.. In the bottom panel, click the *Configuration* tab.
.. Make sure that the *Red Hat GA* repository is in the *Ordered Group Repositories* panel:
+
image::images/nexus-redhat-repo.png[]
+
.. Click *Save*.

== `policyquote` Sample Application

This lab uses a sample application called  `policyquote`.

The `policyquote` application is a fairly simple JBoss BRMS application that calculates the price of a car insurance policy based on driver and car data. The project consists of a number of rules (including a rule flow process) and a domain model in a single Maven project.

[NOTE]
The S2I build mechanism imposes certain limitations on the project structure. Multi-module Maven projects are not well supported. Specifically for kjars, all dependencies (like a domain model jar) should be available in a Maven repository before the build kicks off.

When using binary deployments, you have more flexibility on how to structure your project.

In this part of the lab, you clone the `policyquote` project from GitHub, and push it into the Gogs server on OpenShift to act as the source for your S2I builds.

. In the virtual machine, open a terminal and change to the lab `home` folder.
+
[source,text]
----
$ cd /home/jboss/lab
----
. Clone the `policyquote` project from this course's GitHub site:
+
[source,text]
----
$ git clone https://github.com/gpe-mw-training/bxms-xpaas-policyquote
----
. Add a remote repository to the cloned project that points to your Gogs Git server:
+
[source,text]
----
$ cd bxms-xpaas-policyquote
$ git remote add gogs-s2i http://<gogs username>:<gogs password>@<url of the gogs route>/decision-server-s2i/policyquote.git
----

* Replace `<gogs password>`,`<url of the gogs route>` and `<gogs username>` with the appropriate values for your environment.

. Push the code to the Gogs server:
+
[source,text]
----
$ git push gogs-s2i master
----
. In your browser, return to the home page of your `decision-server-s2i` repository hosted in your `gogs` container:
+
image::images/seeded_gogs_repo.png[]
. Observe that your repository is now seeded with the `policyquote` project.
. Review the code and rules in this application.
.. Notice that the project includes a drools _ruleflow_ artifact:  _PolicyQuote.rf_
+
If you were to view this ruleflow file in JBoss Developer Studio (assuming JBDS is installed with the _Integration Stack_ of plugins), you'd see that the ruleflow is as follows:
+
image::images/policy-quote-rule-flow.png[]
.. Study each of the rule files found in this project.
*** What are the names of the rules affiliated with the _calculation_ ruleflow-group ?
*** What are the names of the rules affiliated with the _surcharge_ ruleflow-group ?


== Decision Server S2I Templates

To create Decision Server applications on OpenShift, you can start from a template that you import into your OpenShift project. Because you can have several templates using the same Decision Server image, you can create an image stream for that image so that you can reuse it in several templates.

. In the VM, open a terminal, and change to the directory in the cloned lab project that contains the templates for the Decision Server lab:
+
[source,text]
----
$ cd /home/jboss/lab/bxms-advanced-infrastructure-lab/xpaas/decision-server
----

. Review the `decisionserver-63-is.yaml` definition file.
* This file defines the ImageStream for the Decision Server 6.3 image, hosted in the Red Hat's Docker registry.
* The latest version of this image is 1.3.

. Create the ImageStream for the Decision Server image:
+
[source,text]
----
$ oc create -f decisionserver-63-is.yaml
----

. Review the `decisionserver-basic-s2i.yaml` template. Note the following:
+
* The template defines:

** A BuildConfig for the S2I build. The BuildConfig defines a source build that points to a Git repo, as well as the builder image, through the ImageStream you defined earlier. The build is triggered through a webhook whenever you push new code to the Git repository, or by a change in the builder image.
** An ImageStream for the image created as a result of the build.
** A DeploymentConfig for the pods running the image created as result of the build. The number of replicas is set to `1`.
** A Service for the Decision Server.
** A Route for the Decision Server.
** Parameters:
*** *KIE_CONTAINER_DEPLOYMENT:* Describes which kjars need to be deployed on the Decision Server, in the format `containerId=groupId:artifactId:version|c2=g2:a2:v2`.
*** *KIE_CONTAINER_REDIRECT_ENABLED:* Enables redirect functionality for KIE containers. This should be set to `true` when different versions of the same kjar are to be deployed side-by-side. The default is `true`.
*** *KIE_SERVER_USER:* The username to access the Decision Server REST or JMS interface. The default is `kieserver`.
*** *KIE_SERVER_PASSWORD:* The password to access the Decision Server REST or JMS interface. The default is a generated value.
*** *APPLICATION_NAME:* The name for the application.
*** *HOSTNAME_HTTP:* The custom hostname for the HTTP service route. Leave blank to use the default hostname generated by OpenShift.
*** *SOURCE_REPOSITORY_URL:* The Git source URI for application. A value is required.
*** *SOURCE_REPOSITORY_REF:* The Git branch/tag reference to build. The default is `master`.
*** *CONTEXT_DIR:* The path within the Git project to build. Leave blank to use the root project directory.
*** *GITHUB_WEBHOOK_SECRET:* The GitHub trigger secret. This is added to the webhook URL. The default is a generated value.
*** *GENERIC_WEBHOOK_SECRET:* The generic build trigger secret. This is added to the webhook URL. The default is a generated value.
*** *IMAGE_STREAM_NAMESPACE:* The namespace in which the ImageStreams for Red Hat xPaaS images are installed. These ImageStreams are normally installed in the `openshift` namespace. You  only need to modify this if you have installed the ImageStreams in a different namespace or project (which is the case in this lab).
*** *MAVEN_MIRROR_URL:* The URL of the Maven mirror--that is, the Nexus server.
* This template does not contain a database service
** Decision Server does not use persistence
* Decision Server uses an insecure route--HTTP, not HTTPS
. Import the template into your OpenShift project:
+
[source,text]
----
$ oc create -f decisionserver-basic-s2i.yaml
----

== Create Your Openshift Decision Server Application

Everything is now in place to create a OpenShift application for your BRMS project.

. In the VM, open a terminal and run the following, replacing `<name of your OpenShift project>` with the appropriate value:
+
[source,text]
----
$ export application_name=policyquote-app
$ export source_repo=http://gogs:3000/decision-server-s2i/policyquote.git
$ export nexus_url=http://nexus:8081
$ export kieserver_password=kieserver1!
$ export is_namespace=<name of your OpenShift project>
$ export kie_container_deployment="policyquote=com.redhat.gpte.xpaas:policyquote:1.0-SNAPSHOT"
$ oc new-app --template=decisionserver63-basic-s2i -p KIE_SERVER_PASSWORD=$kieserver_password,APPLICATION_NAME=$application_name,SOURCE_REPOSITORY_URL=$source_repo,IMAGE_STREAM_NAMESPACE=$is_namespace,KIE_CONTAINER_DEPLOYMENT=$kie_container_deployment,KIE_CONTAINER_REDIRECT_ENABLED=false,MAVEN_MIRROR_URL=$nexus_url/content/groups/public/
----
+
* Note that the `KIE_CONTAINER_REDIRECT_ENABLED` environment variable is set to `false`. This means that the name of the KIE container for your application will be `policyquote`, as defined in `KIE_CONTAINER_DEPLOYMENT`.

. Check the progress of the build by locating the builder pod (named `policyquote-app-1-build`) and checking the logs either in the OpenShift console or via the OpenShift CLI:
+
[source,text]
----
$ oc logs -f policyquote-app-1-build
----
+
* Because this is the first build, it takes quite some time. The builder image needs to be downloaded from Red Hat's Docker repository, and the Nexus Maven proxy needs to be seeded with the build dependencies.
* The S2I build takes place in a builder pod named `policyquote-app-1-build`. Check the logs for this pod in the web console, or use the OpenShift CLI.
* At the end of the build cycle, expect to see the following in the builder pod log:
+
[source,text]
----
I0908 06:48:48.042137       1 sti.go:334] Successfully built xpaas/policyqote-app-1:a0ec7e20
I0908 06:48:48.118123       1 cleanup.go:23] Removing temporary directory /tmp/s2i-build455291570
I0908 06:48:48.118178       1 fs.go:156] Removing directory '/tmp/s2i-build455291570'
I0908 06:48:48.139557       1 sti.go:268] Using provided push secret for pushing 172.30.1.250:5000/xpaas/policyqote-app:latest image
I0908 06:48:48.139575       1 sti.go:272] Pushing 172.30.1.250:5000/xpaas/policyqote-app:latest image ...
I0908 06:51:52.519695       1 sti.go:288] Successfully pushed 172.30.1.250:5000/xpaas/policyqote-app:latest
----
+
* The image built by the builder pod is pushed to the OpenShift internal registry. This triggers the deployment of the image.

. Monitor the deployment of the application by locating the pod (named `policyquote-app-1-xxxxx`) and checking the logs either in the OpenShift console or via the OpenShift CLI.
+
* After some time, you see something like the following:
+
[source,text]
----
06:53:27,949 INFO  [org.kie.server.services.impl.KieServerImpl] (EJB default - 1) Container policyquote (for release id com.redhat.gpte.xpaas:policyquote:1.0-SNAPSHOT) successfully started
----
+
* By that time, the service and the route have started, and your Decision Server application is ready to serve requests.
+
image::images/policyquote-application-ose.png[]

== View Decision Server status

. Before you execute your deployed `policyquote` application, investigate the details of the API exposed by the Decision Server:

.. Determine the route to the `policyquote` application deployed to a `decision-server` container in OpenShift:
+
[source,text]
-----
$ oc get route | grep policyquote
-----
.. In your browser, navigate to: `<policyquote app route>/kie-server/docs`.
.. Pay particular attention to the API that accepts a HTTP POST at the following uri:  `server/containers/instances/{id}`
+
image::images/kie-server-api-post.png[]
+
NOTE: Notice the use of the term _containers_ in the URI of the above resource.
The word _container_ is highly overloaded in the world of software.
The use of the word _container_ in this specific context refers to the Decision Server construct: _KIE Container_.
It does not refer to a OpenShift / Kubernetes _container_.

* This resource URI drives the stateless rules engine of the _decision-server_ application.

* The _id_ specified in the resource URI refers to the identifier of the container to invoke. In this case it is called `policyquote`.

. You can test your application using the REST API exposed by the Decision Server, using `curl`:
.. In a terminal window, run the following:
+
[source,text]
----
$ export policyquote_app=<URL of the policyquote app route>
$ export kieserver_password=kieserver1!
----
.. To check the health of the server:
+
[source,text]
----
$ curl -X GET -H "Accept: application/json" --user kieserver:$kieserver_password "$policyquote_app/kie-server/services/rest/server"
----
+
* Response:
+
[source,text]
----
{
  "type" : "SUCCESS",
  "msg" : "Kie Server info",
  "result" : {
    "kie-server-info" : {
      "version" : "6.4.0.Final-redhat-3",
      "name" : "kieserver-policyquote-app-1-xlgac",
      "location" : "http://policyquote-app-1-xlgac:8080/kie-server/services/rest/server",
      "capabilities" : [ "BRM", "KieServer" ],
      "messages" : [ {
        "severity" : "INFO",
        "timestamp" : 1473333794748,
        "content" : [ "Server KieServerInfo{serverId='kieserver-policyquote-app-1-xlgac', version='6.4.0.Final-redhat-3', location='http://policyquote-app-1-xlgac:8080/kie-server/services/rest/server'}started successfully at Thu Sep 08 07:23:14 EDT 2016" ]
      } ],
      "id" : "kieserver-policyquote-app-1-xlgac"
    }
  }
}
----
.. To check which KIE containers are deployed on the server:
+
[source,text]
----
$ curl -X GET -H "Accept: application/json" --user kieserver:$kieserver_password "$policyquote_app/kie-server/services/rest/server/containers"
----
Response:
+
[source,text]
----
{
  "type" : "SUCCESS",
  "msg" : "List of created containers",
  "result" : {
    "kie-containers" : {
      "kie-container" : [ {
        "status" : "STARTED",
        "messages" : [ {
          "severity" : "INFO",
          "timestamp" : 1473333804577,
          "content" : [ "Container policyquote successfully created with module com.redhat.gpte.xpaas:policyquote:1.0-SNAPSHOT." ]
        } ],
        "container-id" : "policyquote",
        "release-id" : {
          "version" : "1.0-SNAPSHOT",
          "group-id" : "com.redhat.gpte.xpaas",
          "artifact-id" : "policyquote"
        },
        "resolved-release-id" : {
          "version" : "1.0-SNAPSHOT",
          "group-id" : "com.redhat.gpte.xpaas",
          "artifact-id" : "policyquote"
        },
        "config-items" : [ ]
      } ]
    }
  }
}
----

=== Test `policyquote` application

. To test your application, you need to send a correctly formatted payload. The `/xpaas/decision-server` directory of the lab contains an example, formatted as JSON.
.. Open the `policyquote-payload.json` payload file and study its contents.

* Notice how the various batch commands found in this json payload file correspond to similar java _Command_ objects found in the `rule` and `process` directories described in the following links:
** https://github.com/droolsjbpm/drools/tree/master/drools-core/src/main/java/org/drools/core/command/runtime/rule[rule commands]
**https://github.com/droolsjbpm/drools/tree/master/drools-core/src/main/java/org/drools/core/command/runtime/process[process commands].
.. Make sure you are in the `xpaas/decision-server` directory, and run the following:
+
[source,text]
----
$ curl -s -X POST -H "Content-Type: application/json" -H "Accept: application/json" --user kieserver:$kieserver_password -d @policyquote-payload.json "$policyquote_app/kie-server/services/rest/server/containers/instances/policyquote"
----
+
Response:
+
[source,text]
----
{
  "type": "SUCCESS",
  "msg": "Container policyquote successfully called.",
  "result": {
    "execution-results": {
      "results": [
        {
          "key": "driver",
          "value": {
            "com.redhat.gpte.policyquote.model.Driver": {
              "id": "1",
              "driverName": "John Doe",
              "age": 26,
              "ssn": "789456",
              "dlNumber": "123456",
              "numberOfAccidents": 2,
              "numberOfTickets": 1,
              "creditScore": 0
            }
          }
        },
        {
          "key": "policy",
          "value": {
            "com.redhat.gpte.policyquote.model.Policy": {
              "requestDate": null,
              "policyType": "AUTO",
              "vehicleYear": 1999,
              "price": 300,
              "priceDiscount": 0,
              "driver": "1"
            }
          }
        }
      ],
      "facts": [
        {
          "key": "driver",
          "value": {
            "org.drools.core.common.DefaultFactHandle": {
              "external-form": "0:1:725414105:725414105:1:DEFAULT:NON_TRAIT:com.redhat.gpte.policyquote.model.Driver"
            }
          }
        },
        {
          "key": "policy",
          "value": {
            "org.drools.core.common.DefaultFactHandle": {
              "external-form": "0:2:1271576022:1271576022:3:DEFAULT:NON_TRAIT:com.redhat.gpte.policyquote.model.Policy"
            }
          }
        }
      ]
    }
  }
}
----
+
NOTE: In the response, pay particular attention to the `price` field of the Policy. This is set as a result of the execution of the rules in your application.

.. To filter out the `price` field, use `grep`:
+
[source,text]
----
$ curl -s -X POST -H "Content-Type: application/json" -H "Accept: application/json" --user kieserver:$kieserver_password -d @policyquote-payload.json "$policyquote_app/kie-server/services/rest/server/containers/instances/policyquote" | grep '"price"'
----
+
[source,text]
----
  "price" : 300,
----
. Experiment by changing some values in the `policyquote-payload.json` payload file for the Driver and Policy objects and checking if you get another result from the server. Review the rules in the project to get an idea of the fields you need to change to influence the calculated price.

== Application Life Cycle

Here you introduce a change in one of the rules of your application, and observe what happens when you push the change to the Git repository.

=== Configure a Webhook to the OpenShift BuildConfig Object

First you need to define a webhook in your `policyquote` repository on Gogs. This webhook is triggered by a push of new code. The webhook calls the OpenShift API to start a new S2I build.

. In a terminal window, run the following:
+
[source,text]
----
oc describe bc policyquote-app
----

. From the response, copy the URL of the GitHub webhook. It should look like the following:
+
[source,text]
----
https://<OpenShift URL>:8443/oapi/v1/namespaces/xpaas/buildconfigs/policyquote-app/webhooks/<secret>/github
----
. Open a browser window and navigate to the `policyquote` repository on Gogs.
. Click *Settings* at the upper right.
+
image::images/gogs-repository-settings.png[]
. On the *Settings* screen,, click *Webhooks > Add Webhook*.
. Click *Gogs*.
. Paste the webhook URL obtained from the BuildConfig into the *Payload URL* text box.
+
* Leave *Content Type* as `application/json`.
* Leave *Secret* blank.
* Make sure *Just the push event* is selected.
* Make sure the *Active* box is checked.
. Click *Add Webhook*.

=== Change the Rules and Test Your Change

. In a terminal window, change to the root of the cloned `bxms-xpaas-policyquote` project.

. Open the `src/main/resources/RiskyAdults.drl` file for editing.
. Change the price in the rule action to `350`.
+
* The rule should now look like:
+
[source,text]
----
package com.redhat.gpte.policyquote;

import com.redhat.gpte.policyquote.model.Driver
import com.redhat.gpte.policyquote.model.Policy

rule "RiskyAdults"

    ruleflow-group "calculation"

    when
        //conditions
        $driver : Driver(age > 24, numberOfAccidents >= 1 || numberOfTickets >=2, $id : id)
        $policy : Policy(price == 0, policyType == "AUTO", driver == $id)
    then
        //actions
        modify($policy) {setPrice(350)};

end
----
. As the project contains some unit tests for your rules, you need to make a change there as well. Open the `src/test/java/com/redhat/gpte/policyquote/rules/RiskyAdultsTest.java` file for editing.
. Change the assert around line 62 to:
+
[source,text]
----
Assert.assertEquals(350, policy.getPrice().intValue());
----
. Optionally, you can test if the project builds sucessfully by doing a local Maven build:
+
[source,text]
----
$ mvn clean package
----
. After the build has completed, push the changes to the Gogs Git server:
+
[source,text]
----
$ git add --all
$ cat << EOF > ~/.gitconfig
[user]
email = gptestudent@gptestudent.com
name = gptestudent
EOF
$ git commit -m "raised the price for risky adults"
$ git push gogs-s2i master
----
. Check in the OpenShift web console that a new build is triggered by the code push:
+
image::images/openshift-s2i-new-build.png[]
+
* Note that this build does not take as long as the first one.
* After the new build is completed, the original application pod is torn down, while the new build pod is being deployed:
+
image::images/openshift-s2i-new-deployment.png[]

. Test the new deployment:
.. Change the directory to: `~/lab/bxms-advanced-infrastructure-lab/xpaas/decision-server`.
.. Run the following:
+
[source,text]
----
curl -s -X POST -H "Content-Type: application/json" -H "Accept: application/json" --user kieserver:$kieserver_password -d @policyquote-payload.json "$policyquote_app/kie-server/services/rest/server/containers/instances/policyquote" | grep '"price"'
----
+
[source,text]
----
  "price" : 350,
----
* The price should now be `350` instead of `300`.

=== Scaling out and Rolling Deployment

As you have noticed during the build and deployment triggered by a code change, there is a time span during which the application is unavailable. This happens roughly between the moment that the S2I build is finished and the new deployment becomes active. This includes the time needed by the Decision Server to start up.

This downtime is not that serious in a development phase, but it is probably not acceptable in a production environment.

By scaling out your application and defining a rolling upgrade strategy, you can ensure that your application remains available--even if that means that during a limited time span both the old and new versions are deployed concurrently.

Here you introduce the changes required directly in the DeploymentConfig of your application. Alternatively, you could create the changes in the template, load it into the OpenShift project, tear down the existing application, and create a new one based on the modified template.

. In a terminal window, run the following:
+
[source,text]
----
$ oc edit dc policyquote-app
----
+
* This opens the DeploymentConfig definition in YAML format in vi.
+
. Alternatively, if you are unfamiliar with vi, you can also edit the DeploymentConfig directly in the OpenShift web console:
+
.. Navigate to the `policyquote` deployment
.. Click *Actions*.
.. Click *Edit YAML*. This opens a window in which you can edit the YAML file.
. Change the `spec/replicas` and the `spec/strategy` section to match the following content. Note that YAML is indentation-sensitive.
+
[source,text]
----
spec:
  replicas: 2
[...]
  strategy:
    recreateParams:
      timeoutSeconds: 600
    resources: {}
    rollingParams:
      maxSurge: 1
      maxUnavailable: 1
      timeoutSeconds: 600
    type: Rolling
[...]
----
+
* These changes raise the number of required pods for your application to two and define a rolling deployment strategy.
* During deployment, one pod at most is made unavailable (as defined in `maxUnavailable`), and one pod at most is created on top of the replica count (as defined in `maxSurge`).

. Save the file.
* A new policy quote application pod is deployed, bringing the number of pods to 2:
+
image::images/policyquote-deployment-scaled.png[]
+
* Requests to the application are now balanced between the two pods.

. Use curl to test that your application is working correctly. Repeat the steps in the earlier procedure to make a change in the code of the application:
.. This time, change the price in the `Risky Adult` rule to `400`.
.. Remember to change the unit test accordingly.
.. Build locally, commit, and push the change.

. To monitor the availability of the application, use the curl command in a loop:
+
[source,text]
----
$ while [ true ]; do curl -s -X POST -H "Content-Type: application/json" -H "Accept: application/json" --user kieserver:$kieserver_password -d @policyquote-payload.json "$policyquote_app/kie-server/services/rest/server/containers/instances/policyquote" | grep '"price"'; sleep 2; done
----
* When the build is finished, the rolling deployment starts deploying the new application pods.
* As long as at least one of the new pods is not active, the old pod is not torn down:
+
image::images/policyquote-deployment-rolling.png[]
+
* If you launched the curl command in a loop, you should have noticed no interruption in the responsiveness of the application. When the new application pods become active, the application responds with a price of `400` rather than `350`.

== Clean Up the Lab Environment

This concludes the first lab of this module. To save resources on OpenShift, you can tear down the `policyquote` application.

. Leave the Nexus and Gogs applications running, as you need them for the next lab.
. In a terminal window, run the following:

[source,text]
----
$ oc delete dc policyquote-app
$ oc delete service policyquote-app
$ oc delete route policyquote-app
$ oc delete is policyquote-app
$ oc delete bc policyquote-app
$ for pod in `oc get pod | grep "\-build" | awk '{print $1}'`; do oc delete pod $pod; done
----

ifdef::showscript[]

nice job with decision server lab.  still going through it.  really like the use of the ConfigMap object for the gogs server
i think it would be valuable to:
1)  point out to students that there are existing decision server templates in the openshift namespace
2)  our rationale for not leveraging those templates directly as is.  Sounds like one reason is the desire to isolate and re-use the decision server image stream (edited)

Actually there are a couple of reasons to use our own template and image stream:
* The imagestream and templates are not installed by default on OCP < 3.3 (at least not the latest versions)
* The templates in the openshift namespace miss the MAVEN_MIRROR parameter, which makes it a lot harder to leverage nexus as a maven proxy
* The templates in the openshift namespace have a lot of parameters (especially the process server templates) that are not required and might confuse students
* In general I think in real life most people will come up with templates customized to their needs, rather than using the provided ones.
These should be more considered as examples or blueprints.


2)  study and elaborate on:
  - KIE_CONTAINER_DEPLOYMENT
  - KIE_CONTAINER_REDIRECT_ENABLED

endif::showscript[]
