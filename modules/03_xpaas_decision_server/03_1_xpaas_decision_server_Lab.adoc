:scrollbar:
:data-uri:
:toc2:
:numbered:

= xPaaS Decision Server Lab

*Goals*

In this module of the course, we will explore the Red Hat xPaas Decision Server. The Decision Server runtime is a BRMS KIE Server configured for deployment on OpenShift. The Decision Server supports source-to-image builds, as well as binary deployments starting from pre-built kjars.

This module of the course consists of two labs:

* exploring the source-to-image (S2I) functionality of the OpenShift xPaas Decision Server.
* example of binary deployment with the OpenShift xPaaS Decision Server. In this scenario the kjar to depoy is built by a build server, which then triggers a binary deployment on the Decision Server.

*Prerequisites*

In order to do this lab you will need :

* access to a project on a Openshift V3 (OCP) hosted environment.
* required tools on your local workstation: OpenShift CLI client (`oc`), git, maven, curl. +
The lab virtual machine used in this course has these tools installed.
* access to Github to clone the projects used in this lab.

== Lab Set-Up

=== OpenShift
Before starting the lab, log into the OpenShift environment using the OpenShift CLI tool. If using a hosted environment, use the credentials provided to you:

----
$ oc login <URL to Openshift V3 environment>
----

On the environment provided to you, a project has been created. After logging in, all CLI commands will target this project.

Also, make sure you can access the Openshift Web Console. Open a browser, and navigate to the Openshift V3 URL. Login using the same credentials as used for the CLI.

=== Lab Assets

In a previous lab, you should have already cloned the lab assets for this course to your virtual machine.
If not, execute the following:

. In a terminal window, change directory to the _lab_ directory of your virtual machine.
. Execute:
+
-----
$ git clone https://github.com/gpe-mw-training/bxms-advanced-infrastructure-lab.git
-----

== Gogs Git Server

=== Install

For all _xPaas_ labs in this course, we need a Git server where we will host the code that will be built and deployed on the xPaas Decision Server and Process Server. For this we will use _Gogs_, a Github-like Git server, written in Go (https://gogs.io/).

. In the virtual machine, open a terminal, and change to the directory in the cloned lab project that contains the common templates for the XPaaS labs.
+
----
$ cd /home/jboss/lab/bxms-advanced-infrastructure-lab/xpaas/common
----
. Review the `xpaas-gogs-persistent.yaml` template. This is a template for an installation of Gogs backed by a PostgreSQL database. +
The template defines:
* A Service for the Gogs server and the PostgreSQL server.
* A Route for the Gogs server.
* An ImageStream for the Gogs image. This image is hosted on DockerHub.
* A DeploymentConfig for the Gogs pod.
* A DeploymentConfig for the PostgreSQL pod. The data directory of PostgreSQL is mounted as a volume.
* A PersistentVolumeClaim for the Gogs volume
* A PersistentVolumeClaim for the PostgreSQL volume.
* Parameters:
** *APPLICATION_NAME* : the name for the application, defaults to `gogs`.
** *POSTGRESQL_USER* : the name of the user for the PostgreSQL database, generated.
** *GOGS_POSTGRESQL_PASSWORD* : the password of the user for the PostgreSQL database, generated.
** *VOLUME_CAPACITY* : the volume capacity for the PersistentVolumeClaim, defaults to 512 Mi.

. Create an application based on the template. Specify values for the parameters, if you don't want to use the defaults:
+
----
$ oc process -f xpaas-gogs-persistent.yaml -v APPLICATION_NAME=gogs,POSTGRESQL_USER=gogs,GOGS_POSTGRESQL_PASSWORD=gogs,VOLUME_CAPACITY=512Mi | oc create -f -
----

. Wait a few minutes for the _gogs_ and _postgreslq-gogs_ containers to build and deploy.  Soon enough, you should see only these two containers with a status of _Running_:
+
-----
$ oc get pods
NAME                      READY   STATUS    RESTARTS   AGE
gogs-1-89oy3              1/1     Running   0          3m
postgresql-gogs-1-ctngm   1/1     Running   0          4m
-----

=== Understand the code
Research and attempt to answer the following questions:

. What is the full URL to where the _Gogs_ image used in this lab is hosted? How did you determine that and what information does the homepage of the _Gogs_ image provide?
. What is the port exposed by the service to the postgresql container that the _Gogs_ application connects to ?

ifdef::showscript[]

1)https://hub.docker.com/r/openshiftdemos/gogs/
  - ImageStream of DockerImage is:  openshiftdemos/gogs:latest    ..... which implies Dockerhub.
  - URL provides link to source code of gogs image used for OCP
2)  5432

endif::showscript[]

=== Set-up
Once all OpenShift resources are up, we need to setup the Gogs server.

The Gogs configurations are stored in a file within the running container at:  /etc/gogs/conf/app.ini .

We'll make the initial configuration changes (via a web UI).
We'll then ensure that those changes are made permanent such that if/when a new _gogs_ container replaces this existing one, those config changes continue to be utilized.

==== Modify entries in /etc/gogs/conf/app.ini

. Determine the URL of your _Gogs_ server:
+
-----
$ oc get route
-----
.  Open a browser, and navigate to the URL of the gogs route. +
You should be greeted by the Gogs installation screen:
+
image::images/gogs-installation-screen.png[]
. Fill in the following values:
* *Database type* : PostgreSQL
* *Database Host :* postgresql-gogs:5432
* *Database user:* gogs
* *Database password:* gogs
* *Database name :* gogs
* *SSL Mode:* disable
* *Application URL:* http://<gogs route>
* Leave all other settings as is
. Click `Install Gogs`. +
You are redirected to the Sign in screen. +
Leave the browser window open for now.

. Find the name of the Gogs pod:
+
----
$ gogspod=$(oc get pod | grep "^gogs" | awk '{print $1}')
----
. Review the changes made to the _gogs_ configuration file in the existing container:
+
-----
$  oc exec $gogspod -- cat /etc/gogs/conf/app.ini | more

...

ROOT_URL = http://gogs-bxmsadvdserver.cloudapps.test-ml.opentlc.com/

...

DB_TYPE  = postgres
HOST     = postgresql-gogs:5432
NAME     = gogs
USER     = gogs
PASSWD   = gogs

...

-----

==== Make _gogs_ config changes permanent

As a next step, we can extract this configuration file from the Gogs pod, and mount it as a ConfigMap in the container to make it persistent.

. Create a local file with the contents of the `/etc/gogs/conf/app.ini` file:
+
----
$ oc exec $gogspod -- cat /etc/gogs/conf/app.ini > gogs-app.ini
----
. Create a ConfigMap from the saved file:
+
----
$ oc create configmap gogs --from-file=gogs-app.ini
----
. We need to configure Gogs to be able to work with the default self-signed OpenShift certificates. Execute the following command:
+
----
$ sed -i 's/SKIP_TLS_VERIFY = false/SKIP_TLS_VERIFY = true/g' gogs-app.ini
----
. Mount the configmap as a volume in the Gogs pod:
+
----
$ oc set volume dc/gogs --add --overwrite --name=config-volume -m /etc/gogs/conf/ --source='{"configMap":{"name":"gogs","items":[{"key":"gogs-app.ini","path":"app.ini"}]}}'
----
+
Note that this will cause a redeployment of the Gogs pod.
. Wait until the _gogs_ pod has been re-created and is in a status of:  _RUNNING_.
. Create an account and a repository on the Gogs server.
.. Return back to the Gogs login page in your browser.
.. Click on the `Register` link.
+
image::images/gogs_register.png[]

. Create an account. Remember the username and password combination.
. Log in with your username/password combination.
. Create an organization named 	`decision-server-s2i`.
.. Click on the `+` symbol in the upper right, and select `New Organization`.
+
image::images/gogs_new_org.png[]
.. Fill in the name, and click the `Create Organization` button. +
.. Check that you are a member of the new organization. You should be listed as `owner`.
. Create a repository in the `decision-server-s2i` organization the with name `policyquote`. Make sure the repository is not private. Make sure the checkbox `Initialize this repository with selected file and template` is unchecked. Click `Create repository`. +
+
image::images/create_new_repo.png[]
+
Later in the lab we will push our BRMS project to this repository.



== Nexus maven repository server

The S2I build of the Decision Server relies heavily on maven to build and deploy the BRMS project source code. To avoid having to download the maven dependencies at every build cycle, we can configure a Nexus repository as a proxy. The maven build will download the dependencies it needs from the
Nexus proxy rather than the internet, which will drastically improve the build speed.

In this section we will install and configure a Nexus server in our OpenShift project.

. In the virtual machine, open a terminal, and change to the directory in the cloned lab project that contains the common templates for the XPaaS labs.
+
----
$ cd /home/jboss/lab/bxms-advanced-infrastructure-lab/xpaas/common
----
. Review the `xpaas-nexus-persistent.yaml` template. This is a template for the installation of Nexus. +
The template defines:
* A Service for the Nexus server.
* A Route for the Nexus server.
* An ImageStream for the Nexus docker image. This image is hosted on DockerHub.
* A DeploymentConfig for the Nexus pod.
* A PersistentVolumeClaim for the Nexus volume, to hold the Nexus configuration and storage.
* Parameters:
** APPLICATION_NAME : the name for the application, defaults to `nexus`.
** VOLUME_CAPACITY : the volume capacity for the PersistentVolumeClaim, defaults to 512 Mi.

. Create an application based on the template. Specify values for the parameters, if you don't want to use the defaults:
+
----
$ oc process -f xpaas-nexus-persistent.yaml -v APPLICATION_NAME=nexus,VOLUME_CAPACITY=512Mi | oc create -f -
----

. Once all components of our application are up, configure the Nexus server. More specifically, we need to add the Red Hat enterprise maven repository to the list of proxied repo's.
. In a browser window, navigate to the URL of the Nexus route.
. Log in with the `admin/admin123` username/password.
. Click on the `Repositories` on the left menu, and next on the `Add...` icon in the top menu. Choose to create a `Proxy Repository`
. In the `New Proxy Repository` form, fill in the following values:
** Repository ID: redhat-ga
** Repository Name: Red Hat GA
** Remote Storage Location : https://maven.repository.redhat.com/ga/
** Leave the other fields as is.
** Click `Save`
. Add the Red Hat GA repository to the public repository group.
** Click on the `Repositories` on the left menu, and then on the `Public Repositories` in the list of repositories.
** In the bottom pane, click on the `Configuration` tab.
** Make sure that the `Red Hat GA` repository is in the `Ordered Group Repositories` pane.
+
image::images/nexus-redhat-repo.png[]
+
** Click `Save`.

=== Decision Server S2I Template

To create Decision Server applications on OpenShift, we can start from a template that we will import into our OpenShift project. As we can have several templates using the same Decision Server image, we will first create an image stream for the Decision Server image, so that we can reuse the image stream in several templates.

. In the virtual machine, open a terminal, and change to the directory in the cloned lab project that contains the templates for the Decision Server lab.
+
----
$ cd /home/jboss/lab/bxms-advanced-infrastructure-lab/xpaas/decision-server
----
. Review the `decisionserver-63-is.yaml` definition file. This file defines the ImageStream for the Decision Server 6.3 image, hosted in the Red Hat docker registry. The latest version of this image is 1.3.
. Create the ImageStream for the Decision Server image:
+
----
$ oc create -f decisionserver-63-is.yaml
----
. Review the `decisionserver-basic-s2i.yaml` template. +
This template defines:
* A BuildConfig for the S2I build. +
The BuildConfig defines a source build, pointing to a git repo, as well as the builder image, through the ImageStream we defined earlier. +
The build will be triggered through a webhook (triggered whenever we push new code to the git repository), or by a change in the builder image.
* An ImageStream for the image created as a result of the build.
* A DeploymentConfig for the pod(s) running the image created as result of the build. The number of replica's is set to one.
* A Service for the Decision Server.
* A Route for the Decision Server.
* Parameters:
** KIE_CONTAINER_DEPLOYMENT : describes what kjar(s) needs to be deployed on the Decision Server, in the format `containerId=groupId:artifactId:version|c2=g2:a2:v2`
** KIE_CONTAINER_REDIRECT_ENABLED : Enable redirect functionality for KIE containers. Defaults to true. Should be true when different versions of the same kjar are to be deployed side-by-side.
** KIE_SERVER_USER : the user name to access the KIE Server REST or JMS interface. Defaults to `kieserver`.
** KIE_SERVER_PASSWORD : The password to access the KIE Server REST or JMS interface. Defaults to a generated value.
** APPLICATION_NAME : the name for the application.
** HOSTNAME_HTTP : Custom hostname for the http service route. Leave blank for default hostname generated by OpenShift.
** SOURCE_REPOSITORY_URL : Git source URI for application. Required.
** SOURCE_REPOSITORY_REF : the Git branch/tag reference to build. Defaults to `master`.
** CONTEXT_DIR : The path within the Git project to build. Leave blank for the root project directory.
** GITHUB_WEBHOOK_SECRET : GitHub trigger secret. Will be added to the webhook URL. Defaults to a generated value.
** GENERIC_WEBHOOK_SECRET: Generic build trigger secret. Will be added to the webhook URL. Defaults to a generated value.
** IMAGE_STREAM_NAMESPACE : Namespace in which the ImageStreams for Red Hat xPaaS images are installed. These ImageStreams are normally installed in the openshift namespace. You should only need to modify this if you've installed the ImageStreams in a different namespace/project (which is the case in our lab).
** MAVEN_MIRROR_URL : The URL of the maven mirror (Nexus server)
* Note: this template does not contain a database service (Decision Server does not use persistence). The Decision Server uses an insecure route (http, no https).
. Import the template into your OpenShift project:
+
----
$ oc create -f decisionserver-basic-s2i.yaml
----

== Policyquote sample application

The Policyquote sample application is a fairly simple BRMS application to calculate the price of a car insurance policy based on driver and car data. The project consists of a number of rules (including a ruleflow process), and a domain model in a single maven project.

[NOTE]
The S2I build mechanism imposes certain limitations on the project structure. Multi-module maven projects are not well supported. Specifically for kjars, all dependencies (like a domain model jar) should be available in a maven repository before the build kicks off. +
When using binary deployments, you have more flexibility on how to structure your project.

In this part of the lab, we will clone the Policyquote project from Github, and push it into our Gogs server on OpenShift to act as source for our S2I builds.

. In the virtual machine, open a terminal and change to the lab home folder.
+
----
$ cd /home/jboss/lab
----
. Clone the Policyquote project from the GPTE Github site:
+
----
$ git clone https://github.com/gpe-mw-training/bxms-xpaas-policyquote
----
. Add a remote repository to the cloned project pointing to our Gogs git server:
+
----
$ cd bxms-xpaas-policyquote
$ git remote add gogs http://<gogs username>:<gogs password>@<url of the gogs route>/decision-server-s2i/policyquote.git
----
+
Replace `<gogs password>`,`<url of the gogs route>` and `<gogs username>` with the appropriate values for your environment.
. Push the code to the Gogs server:
+
----
$ git push gogs master
----

== Openshift Decision Server Application

Everything is in place now to create a OpenShift application for our BRMS project.

. In the virtual machine, open a terminal, and issue the following commands (replace expressions between `<>` with correct values for your environment):
+
----
$ application_name=policyquote-app
$ source_repo=http://gogs:3000/decision-server-s2i/policyquote.git
$ nexus_url=http://nexus:8081
$ kieserver_password=kieserver1!
$ is_namespace=<name of your OpenShift project>
$ kie_container_deployment="policyquote=com.redhat.gpte.xpaas:policyquote:1.0-SNAPSHOT"
$ oc new-app --template=decisionserver63-basic-s2i -p KIE_SERVER_PASSWORD=$kieserver_password,APPLICATION_NAME=$application_name,SOURCE_REPOSITORY_URL=$source_repo,IMAGE_STREAM_NAMESPACE=$is_namespace,KIE_CONTAINER_DEPLOYMENT=$kie_container_deployment,KIE_CONTAINER_REDIRECT_ENABLED=false,MAVEN_MIRROR_URL=$nexus_url/content/groups/public/
----
+
* Note that the KIE_CONTAINER_REDIRECT_ENABLED environment variable is set to false. This means that the name of the KIE-Container for our application will be `policyquote`, as defined in KIE_CONTAINER_DEPLOYMENT.

. Check the progress of the build and deployment of the application in the OpenShift console.
* As this is the first build, it will take quite some time: the builder image needs to be downloaded from the Red Hat docker repository, and the Nexus maven proxy needs to be seeded with the build dependencies.
* The S2I build is happening in a builder pod, named `policyquote-app-1-build`. Check the logs for this pod in the web console, or use the Openshift CLI:
+
----
$ oc logs -f policyquote-app-1-build
----
* At the end of the build cycle, you should see the following in the builder pod log:
+
----
I0908 06:48:48.042137       1 sti.go:334] Successfully built xpaas/policyqote-app-1:a0ec7e20
I0908 06:48:48.118123       1 cleanup.go:23] Removing temporary directory /tmp/s2i-build455291570
I0908 06:48:48.118178       1 fs.go:156] Removing directory '/tmp/s2i-build455291570'
I0908 06:48:48.139557       1 sti.go:268] Using provided push secret for pushing 172.30.1.250:5000/xpaas/policyqote-app:latest image
I0908 06:48:48.139575       1 sti.go:272] Pushing 172.30.1.250:5000/xpaas/policyqote-app:latest image ...
I0908 06:51:52.519695       1 sti.go:288] Successfully pushed 172.30.1.250:5000/xpaas/policyqote-app:latest
----
* The image built by the builder pod is pushed to the OpenShift internal registry. This will trigger the deployment of the image.
* To check the logs of the application pod, locate the pod (name `policyquote-app-1-xxxxx`), and check the logs in the OpenShift console or with the CLI.
* After some time, you will see something like:
+
----
06:53:27,949 INFO  [org.kie.server.services.impl.KieServerImpl] (EJB default - 1) Container policyquote (for release id com.redhat.gpte.xpaas:policyquote:1.0-SNAPSHOT) successfully started
----
* By that time, the service and the route will be started, and our Decision Server application is ready to serve requests.
+
image::images/policyquote-application-ose.png[]

. We will test our application using the REST API exposed by the Decision Server, using `curl`. +
In a terminal window, issue the following commands:
+
----
$ policyquote_app=<URL of the policyquote app route>
$ kieserver_password=kieserver1!
----
. To check the health of the server:
+
----
$ curl -X GET -H "Accept: application/json" --user kieserver:$kieserver_password "$policyquote_app/kie-server/services/rest/server"
----
+
Response:
+
----
{
  "type" : "SUCCESS",
  "msg" : "Kie Server info",
  "result" : {
    "kie-server-info" : {
      "version" : "6.4.0.Final-redhat-3",
      "name" : "kieserver-policyquote-app-1-xlgac",
      "location" : "http://policyquote-app-1-xlgac:8080/kie-server/services/rest/server",
      "capabilities" : [ "BRM", "KieServer" ],
      "messages" : [ {
        "severity" : "INFO",
        "timestamp" : 1473333794748,
        "content" : [ "Server KieServerInfo{serverId='kieserver-policyquote-app-1-xlgac', version='6.4.0.Final-redhat-3', location='http://policyquote-app-1-xlgac:8080/kie-server/services/rest/server'}started successfully at Thu Sep 08 07:23:14 EDT 2016" ]
      } ],
      "id" : "kieserver-policyquote-app-1-xlgac"
    }
  }
}
----
. To check which KIE-Containers are deployed on the server:
+
----
$ curl -X GET -H "Accept: application/json" --user kieserver:$kieserver_password "$policyquote_app/kie-server/services/rest/server/containers"
----
Response:
+
----
{
  "type" : "SUCCESS",
  "msg" : "List of created containers",
  "result" : {
    "kie-containers" : {
      "kie-container" : [ {
        "status" : "STARTED",
        "messages" : [ {
          "severity" : "INFO",
          "timestamp" : 1473333804577,
          "content" : [ "Container policyquote successfully created with module com.redhat.gpte.xpaas:policyquote:1.0-SNAPSHOT." ]
        } ],
        "container-id" : "policyquote",
        "release-id" : {
          "version" : "1.0-SNAPSHOT",
          "group-id" : "com.redhat.gpte.xpaas",
          "artifact-id" : "policyquote"
        },
        "resolved-release-id" : {
          "version" : "1.0-SNAPSHOT",
          "group-id" : "com.redhat.gpte.xpaas",
          "artifact-id" : "policyquote"
        },
        "config-items" : [ ]
      } ]
    }
  }
}
----
. To test our application, we need to send a correctly formatted payload. The `/xpaas/decision-server` directory of the lab contains an example, formatted as JSON. Make sure you are in that directory, and execute:
+
----
$ curl -s -X POST -H "Content-Type: application/json" -H "Accept: application/json" --user kieserver:$kieserver_password -d @policyquote-payload.json "$policyquote_app/kie-server/services/rest/server/containers/instances/policyquote"
----
+
Response:
+
----
{
  "type": "SUCCESS",
  "msg": "Container policyquote successfully called.",
  "result": {
    "execution-results": {
      "results": [
        {
          "key": "driver",
          "value": {
            "com.redhat.gpte.policyquote.model.Driver": {
              "id": "1",
              "driverName": "John Doe",
              "age": 26,
              "ssn": "789456",
              "dlNumber": "123456",
              "numberOfAccidents": 2,
              "numberOfTickets": 1,
              "creditScore": 0
            }
          }
        },
        {
          "key": "policy",
          "value": {
            "com.redhat.gpte.policyquote.model.Policy": {
              "requestDate": null,
              "policyType": "AUTO",
              "vehicleYear": 1999,
              "price": 300,
              "priceDiscount": 0,
              "driver": "1"
            }
          }
        }
      ],
      "facts": [
        {
          "key": "driver",
          "value": {
            "org.drools.core.common.DefaultFactHandle": {
              "external-form": "0:1:725414105:725414105:1:DEFAULT:NON_TRAIT:com.redhat.gpte.policyquote.model.Driver"
            }
          }
        },
        {
          "key": "policy",
          "value": {
            "org.drools.core.common.DefaultFactHandle": {
              "external-form": "0:2:1271576022:1271576022:3:DEFAULT:NON_TRAIT:com.redhat.gpte.policyquote.model.Policy"
            }
          }
        }
      ]
    }
  }
}
----
+
Of particular importance in the response is the price field of the Policy, which has been set as a result of the execution of the rules in our application. +
To filter out the price field, use `grep`:
+
----
$ curl -s -X POST -H "Content-Type: application/json" -H "Accept: application/json" --user kieserver:$kieserver_password -d @policyquote-payload.json "$policyquote_app/kie-server/services/rest/server/containers/instances/policyquote" | grep '"price"'
----
+
----
  "price" : 300,
----
. Feel free to change some values in the payload file (`policyquote-payload.json`) for the Driver and Policy objects, and check if you get another result from the server. You can review the rules in the project to have an idea what fields need to be changed to influence the calculated price.

== Application lifecycle

Now we can introduce a change in one of the rules of our application, and observe what's happening when we push the change to the git repository. +
First we need to define a webhook in our policyquote repository on Gogs, that will be triggered by a push of new code. The webhook calls the Openshift API in order to start a new S2I build.

. In a terminal window, issue the following command:
+
----
oc describe bc policyquote-app
----
+
From the response, copy the URL of the GitHub Webhook. This should look like:
+
----
https://<OpenShift URL>:8443/oapi/v1/namespaces/xpaas/buildconfigs/policyquote-app/webhooks/<secret>/github
----
. Open a browser window and navigate to the policyquote repository on Gogs. Click on the `Settings` link in the top left.
+
image::images/gogs-repository-settings.png[]
. In the settings window menu, click on `Webhooks`, and then on `Add Webhook`. Choose the `Gogs` format.
. Paste the webhook URL obtained from the from the BuildConfig into the `Payload URL` text box. +
Leave `Content Type` to application/json, and leave `Secret` blank. +
Make sure the `Just the push event` radio button and the `Active` check box is selected. +
Click `Add Webhook`.
. In a terminal window, change to the root of the cloned `bxms-xpaas-policyquote` project.
+
----
$ cd /home/jboss/lab/bxms-xpaas-policyquote
----
. Open the `src/main/resources/RiskyAdults.drl` file for editing. Change the price in the rule action to 350. +
The rule should now look like:
+
----
package com.redhat.gpte.policyquote;

import com.redhat.gpte.policyquote.model.Driver
import com.redhat.gpte.policyquote.model.Policy

rule "RiskyAdults"

    ruleflow-group "calculation"

    when
        //conditions
        $driver : Driver(age > 24, numberOfAccidents >= 1 || numberOfTickets >=2, $id : id)
        $policy : Policy(price == 0, policyType == "AUTO", driver == $id)
    then
        //actions
        modify($policy) {setPrice(350)};

end
----
. As the project contains some unit tests for our rules, (like it should be, right?), we need to make a change there as well. +
Open the `src/test/java/com/redhat/gpte/policyquote/rules/RiskyAdultsTest.java` for editing. Change the assert around line 62 to:
+
----
Assert.assertEquals(350, policy.getPrice().intValue());
----
. Optionally, you can test if the project builds sucessfully by doing a local maven build:
+
----
$ mvn clean package
----
. If the build succeeds, push the changes to the Gogs git server:
+
----
$ git add --all
$ git commit -m "raised the price for risky adults"
$ git push gogs master
----
. Check in the Openshfift web console that a new build is triggered by the code push.
+
image::images/openshift-s2i-new-build.png[]
+
Note that this build does not take as long as the first one.
. Once the new build is completed, the original application pod is teared down, while the new build pod is being deployed.
+
image::images/openshift-s2i-new-deployment.png[]
. Test the new deployment. Change the directory to the `/xpaas/decision-server` directory of the lab folder first. The price should now be 350 instead of 300.
+
----
curl -s -X POST -H "Content-Type: application/json" -H "Accept: application/json" --user kieserver:$kieserver_password -d @policyquote-payload.json "$policyquote_app/kie-server/services/rest/server/containers/instances/policyquote" | grep '"price"'
----
+
----
  "price" : 350,
----

=== Scaling out and Rolling Deployment

As you will have noticed during the build and deployment triggered by a code change, there is a time span during which the application is unavailable. This happens grosso modo between the moment that the S2I build is finished, and the new deployment is active. This includes the time needed by the Decision Server to start up. +
In a development phase, this is not so dramatic, but it is probably not acceptable in a production environment.
By scaling out our application, and defining a rolling upgrade strategy, we can ensure that our application remains available, even if that means that during a limited time span both the old as the new version will be deployed concurrently.

We are going to introduce the changes required directly in the DeploymentConfig of our application. Alternatively, you could create the changes in the template, load it into the OpenShift project, tear down the existing application and create a new one based on the modified template.

. In a terminal window, execute the following command:
+
----
$ oc edit dc policyquote-app
----
+
This will open the DeploymentConfig definition in YAML format in vi. +
If you are unfamiliar with vi, you can also edit the DeploymentConfig directly in the OpenShift web console. Navigate to the policyquote deployment, click on the `Actions` button in the top left, and choose `Edit YAML`. This will open a popup window in which you can edit the YAML file.
. Change the `spec/replicas` and the `spec/strategy` section to match the following content. Note that YAML is indentation sensitive.
+
----
spec:
  replicas: 2
[...]
  strategy:
    recreateParams:
      timeoutSeconds: 600
    resources: {}
    rollingParams:
      maxSurge: 1
      maxUnavailable: 1
      timeoutSeconds: 600
    type: Rolling
[...]
----
+
We raised the number of required pods for our application to 2, and defined a Rolling deployment strategy. During deployment, at most one pod will be made unavailable (maxUnavailable), and we will create at most one extra pod on top of the replica count (maxSurge).
. Save the file. As a result, a new policy quote application pod will be deployed, bringing the number of pods to 2.
+
image::images/policyquote-deployment-scaled.png[]
+
Requests to the application will now be balanced between the two pods. You can use curl to test that our application is still working fine.
. Repeat the instructions detailed above to make a change in the code of the application. +
This time, change the price in the Risky Adult rule to 400. Don't forget to change the unit test accordingly. Build locally, commit and push the change.
. To monitor the availability of the application, use the curl command in a loop.
+
----
$ while [ true ]; do curl -s -X POST -H "Content-Type: application/json" -H "Accept: application/json" --user kieserver:$kieserver_password -d @policyquote-payload.json "$policyquote_app/kie-server/services/rest/server/containers/instances/policyquote" | grep '"price"'; sleep 2; done
----
. When the build is finished, the rolling deployment will start deploying the new application pods, but as long as at least one of the new pods is not active, the old pod will not be teared down.
+
image::images/policyquote-deployment-rolling.png[]
+
If you launched the curl command in a loop you should haved noticed no interruption in the responsiveness of the application. When the new application pods become active, the application responds with a price of 400 rather than 350.

This concludes the first part of this lab. To save resources on Openshift, you can tear down the policyquote application. Leave the Nexus and Gogs applications running, as we will need them for the remainder of the lab.

. In a terminal window, issue the following commands:
+
----
$ oc delete dc policyquote-app
$ oc delete service policyquote-app
$ oc delete route policyquote-app
$ oc delete is policyquote-app
$ oc delete bc policyquote-app
----

ifdef::showscript[]
endif::showscript[]
